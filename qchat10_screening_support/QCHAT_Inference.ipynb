{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6641e0",
   "metadata": {},
   "source": [
    "# QCHAT Inference Workflow\n",
    "\n",
    "This notebook demonstrates the inference process for evaluating a fine-tuned RoBERTa model on QCHAT-based ASD vs. TD classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizerFast, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Specify the fine-tuned model path\n",
    "fine_tuned_model_path = \"./Roberta_QCHAT_Model\"\n",
    "\n",
    "# Function to load the fine-tuned model and tokenizer\n",
    "def load_fine_tuned_model(model_path):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model, tokenizer = load_fine_tuned_model(fine_tuned_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd791bfb",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30534752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the test dataset\n",
    "def tokenize_and_pad(dataset):\n",
    "    return tokenizer(\n",
    "        dataset['text'].tolist(),  # Use 'text' column for tokenization\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=567  # Set maximum token length\n",
    "    )\n",
    "\n",
    "# Define Dataset class\n",
    "class QCHATDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, subject_ids):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.subject_ids = subject_ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['SubjectId'] = self.subject_ids[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Tokenize the test data and create dataset\n",
    "test_encodings = tokenize_and_pad(test)\n",
    "test_subject_ids = test['SubjectId'].tolist()\n",
    "test_dataset = QCHATDataset(test_encodings, test['label'].tolist(), test_subject_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7efdbb",
   "metadata": {},
   "source": [
    "## Step 2: Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce655be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "# Generate predictions on the test dataset\n",
    "predictions_output = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predictions, probabilities, and true labels\n",
    "predictions = np.argmax(predictions_output.predictions, axis=-1)\n",
    "probabilities = softmax(predictions_output.predictions, axis=-1)\n",
    "true_labels = predictions_output.label_ids\n",
    "\n",
    "# Create a DataFrame for results\n",
    "df_results = pd.DataFrame({\n",
    "    'True_Labels': true_labels,\n",
    "    'Predictions': predictions,\n",
    "    'Probabilities': list(probabilities),\n",
    "    'SubjectId': test_subject_ids\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results.head())\n",
    "\n",
    "# Merge results with the original test dataset\n",
    "test_final = pd.merge(test, df_results, on='SubjectId')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78672e8",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(df_results['True_Labels'], df_results['Predictions'])\n",
    "\n",
    "# Compute classification report\n",
    "class_report = classification_report(df_results['True_Labels'], df_results['Predictions'])\n",
    "\n",
    "# Print classification report\n",
    "print(class_report)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218fc9ed",
   "metadata": {},
   "source": [
    "## Step 4: Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe874680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute true positive rates and false positive rates\n",
    "y_true = np.array(df_results['True_Labels'])\n",
    "y_proba = np.array([proba[1] for proba in df_results['Probabilities']])  # Probabilities for class 1\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "roc_auc = roc_auc_score(y_true, y_proba)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
