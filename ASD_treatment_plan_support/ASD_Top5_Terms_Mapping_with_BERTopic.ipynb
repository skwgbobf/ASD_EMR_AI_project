{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c77458f",
   "metadata": {},
   "source": [
    "# Mapping Top-5 ASD Terms Using OpenAI Embeddings and BERTopic\n",
    "\n",
    "This notebook demonstrates how to use OpenAI embeddings and BERTopic modeling to identify ASD behavior patterns for supporting ASD treatment planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3796f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import textwrap\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('/home/skbae/Documents/skbae/ASD/PJT_Data/Questions/Work/cdb_advanced_ASD_F3.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_text(text):\n",
    "    return \" \".join([word.lower() for word in text.split()])\n",
    "\n",
    "data['Processed_Name'] = data['Name'].apply(preprocess_text)\n",
    "\n",
    "# Create FAISS vector store with OpenAI embeddings\n",
    "vectorstore = FAISS.from_texts(data['Processed_Name'].tolist(), embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Load clinical notes\n",
    "df_merged = pd.read_csv('./df_merged_summary2_LLM_F_Apr012F.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed33d41a",
   "metadata": {},
   "source": [
    "## Step 1: Process Text to Find Top-5 ASD Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eeb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to find the top-5 similar terms\n",
    "max_length = 50\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "def process_text_row(text):\n",
    "    chunks = textwrap.wrap(text, max_length)\n",
    "    if not chunks:\n",
    "        return None, None\n",
    "\n",
    "    chunks_embeddings = embeddings_model.embed_documents(chunks)\n",
    "    processed_name_embeddings = embeddings_model.embed_documents(data['Processed_Name'].tolist())\n",
    "\n",
    "    cosine_scores = util.pytorch_cos_sim(chunks_embeddings, processed_name_embeddings)\n",
    "    scores_numpy = cosine_scores.cpu().numpy()\n",
    "    indices = np.argsort(scores_numpy, axis=1)[:, ::-1]\n",
    "\n",
    "    top_5_similar = data['Processed_Name'].iloc[indices[0, :5]].tolist()\n",
    "    top_5_id = data['ID'].iloc[indices[0, :5]].tolist()\n",
    "\n",
    "    top_5_similar_str = \"', '\".join(top_5_similar)\n",
    "    top_5_id_str = \"', '\".join(top_5_id)\n",
    "\n",
    "    return f\"('{top_5_similar_str}')\", f\"('{top_5_id_str}')\"\n",
    "\n",
    "# Apply the function to the dataset\n",
    "df_merged['Top_5_Similar_Processed_NameF'], df_merged['Top_5_IDF'] = zip(*df_merged['summarized_text_llm2F'].apply(process_text_row))\n",
    "df_merged.to_csv('./df_top5_preprocessing_OPENAI_RAG_summarized_similarity_Apr12F2_openAI_ID_F.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d1ba72",
   "metadata": {},
   "source": [
    "## Step 2: Group and Merge Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the processed data\n",
    "df_openAI = pd.read_csv('./df_top5_preprocessing_OPENAI_RAG_summarized_similarity_Apr12F2_openAI_ID_F.csv', encoding='utf-8')\n",
    "\n",
    "# Fill NaN values\n",
    "df_openAI['Top_5_Similar_Processed_NameF'] = df_openAI['Top_5_Similar_Processed_NameF'].fillna('')\n",
    "df_openAI['Top_5_IDF'] = df_openAI['Top_5_IDF'].fillna('')\n",
    "\n",
    "# Group by 'patient_key' and merge text\n",
    "merged_df = df_openAI.groupby('patient_key')['Top_5_Similar_Processed_NameF'].apply(', '.join).reset_index()\n",
    "merged_df_id = df_openAI.groupby('patient_key')['Top_5_IDF'].apply(', '.join).reset_index()\n",
    "\n",
    "# Save merged results\n",
    "merged_df.to_csv('./merged_top5_ASD_terms.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a30132",
   "metadata": {},
   "source": [
    "## Step 3: BERTopic Modeling for Behavior Pattern Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pre-calculate embeddings for BERTopic\n",
    "docs = merged_df['Top_5_Similar_Processed_NameF'].apply(str).tolist()\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Initialize UMAP and HDBSCAN models\n",
    "umap_model = UMAP(n_neighbors=6, n_components=6, min_dist=0.0, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Create a BERTopic model\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer,\n",
    "    ctfidf_model=ClassTfidfTransformer(),\n",
    "    calculate_probabilities=True,\n",
    "    top_n_words=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit BERTopic on the data\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# Save topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.to_csv('./topic_info_Apr16.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4f31a",
   "metadata": {},
   "source": [
    "## Step 4: Visualization and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606af094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the topics\n",
    "topic_model.visualize_barchart()\n",
    "topic_model.visualize_topics()\n",
    "\n",
    "# Display topic details\n",
    "for i in range(len(topic_model.get_topic_info()) - 1):\n",
    "    topic_words = topic_model.get_topic(i)\n",
    "    print(f\"Topic {i}: {topic_words}\\n\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
